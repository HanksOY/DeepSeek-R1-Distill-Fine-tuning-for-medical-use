{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install unsloth\n!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T23:37:10.123463Z","iopub.execute_input":"2025-05-26T23:37:10.124002Z","iopub.status.idle":"2025-05-26T23:41:45.531161Z","shell.execute_reply.started":"2025-05-26T23:37:10.123974Z","shell.execute_reply":"2025-05-26T23:41:45.530123Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nimport wandb\nfrom unsloth import FastLanguageModel\nfrom datasets import load_dataset\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T23:42:45.887577Z","iopub.execute_input":"2025-05-26T23:42:45.888346Z","iopub.status.idle":"2025-05-26T23:43:33.059392Z","shell.execute_reply.started":"2025-05-26T23:42:45.888281Z","shell.execute_reply":"2025-05-26T23:43:33.058477Z"}},"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-05-26 23:43:06.699994: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748302987.135390      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748302987.257296      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"🦥 Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Inistialize HF and WB tokens\nsecrets = UserSecretsClient() # Create instance via importing UserSecretsClient from kaggle_secrets\nhf_token = secrets.get_secret(\"hf_token\")\nwb_token = secrets.get_secret(\"wb_token\")\n\n#Login to HF\nlogin(hf_token)\n\n#Login to wb\nwandb.login(key=wb_token)\nrun = wandb.init(\n    project='DeepSeek-R1-Distill-Qwen-1.5B for medical use', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T23:43:34.828258Z","iopub.execute_input":"2025-05-26T23:43:34.829058Z","iopub.status.idle":"2025-05-26T23:43:47.758848Z","shell.execute_reply.started":"2025-05-26T23:43:34.829033Z","shell.execute_reply":"2025-05-26T23:43:47.757974Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhanksouyang5155\u001b[0m (\u001b[33mhanksouyang5155-northeastern-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250526_234341-iye2gfve</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/hanksouyang5155-northeastern-university/DeepSeek-R1-Distill-Qwen-1.5B%20for%20medical%20use/runs/iye2gfve?apiKey=8f86d9dcb091e484f533faa0c533e606d51eeb86' target=\"_blank\">zesty-thunder-3</a></strong> to <a href='https://wandb.ai/hanksouyang5155-northeastern-university/DeepSeek-R1-Distill-Qwen-1.5B%20for%20medical%20use?apiKey=8f86d9dcb091e484f533faa0c533e606d51eeb86' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/hanksouyang5155-northeastern-university/DeepSeek-R1-Distill-Qwen-1.5B%20for%20medical%20use?apiKey=8f86d9dcb091e484f533faa0c533e606d51eeb86' target=\"_blank\">https://wandb.ai/hanksouyang5155-northeastern-university/DeepSeek-R1-Distill-Qwen-1.5B%20for%20medical%20use?apiKey=8f86d9dcb091e484f533faa0c533e606d51eeb86</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/hanksouyang5155-northeastern-university/DeepSeek-R1-Distill-Qwen-1.5B%20for%20medical%20use/runs/iye2gfve?apiKey=8f86d9dcb091e484f533faa0c533e606d51eeb86' target=\"_blank\">https://wandb.ai/hanksouyang5155-northeastern-university/DeepSeek-R1-Distill-Qwen-1.5B%20for%20medical%20use/runs/iye2gfve?apiKey=8f86d9dcb091e484f533faa0c533e606d51eeb86</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Do NOT share these links with anyone. They can be used to claim your runs."},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Set parameters\nmax_seq_length = 2048 # Define the maximum sequence length a model can handle (i.e. how many tokens can be processed at once)\ndtype = None # AUtomatically detect by setting to default\nload_in_4bit = True # Enables 4 bit quantization for memory saving optimization\n\n# Load the Deepseek model and tokenizer using unsloth - imported using: from unsloth import FastLanguageModel\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/DeepSeek-R1-Distill-Qwen-1.5B\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    token = hf_token, \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T23:43:47.760223Z","iopub.execute_input":"2025-05-26T23:43:47.761041Z","iopub.status.idle":"2025-05-26T23:44:30.570987Z","shell.execute_reply.started":"2025-05-26T23:43:47.761014Z","shell.execute_reply":"2025-05-26T23:44:30.570266Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.5.7: Fast Qwen2 patching. Transformers: 4.51.3.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.81G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f8c29137a894b5d8584cf4356717c43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/236 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"303a2a6a3a314e8285e0d3d5f4dd97c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/6.78k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16427692d4a449c594570222c0dcb77a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac9383fc2a364f99bf034e7c575d2449"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c56ad0a9c0e74bc786ebff41d78d3c36"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Define a system prompt using prompt_style\nprompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the request. \nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \nPlease answer the following medical question. \n\n### Question:\n{}\n\n### Response:\n<think>{}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T23:45:23.542250Z","iopub.execute_input":"2025-05-26T23:45:23.542893Z","iopub.status.idle":"2025-05-26T23:45:23.548073Z","shell.execute_reply.started":"2025-05-26T23:45:23.542866Z","shell.execute_reply":"2025-05-26T23:45:23.547200Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Create a test medical question for inference\nquestion = \"\"\"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing\n                but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would \n                cystometry most likely reveal about her residual volume and detrusor contractions?\"\"\"\n\n# Enable optimized inference mode for Unsloth models (improves speed and efficiency)\nFastLanguageModel.for_inference(model)\n\n# FOrmat the question using the structured prompt (\"prompt_style\") and tokenize it\ninputs = tokenizer([prompt_style.format(question, \"\")], return_tensors = \"pt\").to(\"cuda\")\n\n# Generate a response using the model\noutputs = model.generate(\n    input_ids = inputs.input_ids,\n    attention_mask = inputs.attention_mask,\n    max_new_tokens = 1200,\n    use_cache = True\n)\n\n# Decode the generated output tokens into readable text\nresponse = tokenizer.batch_decode(outputs)\n\n# Extract and print only relavent response part (after \"### Response:\")\nprint(response[0].split(\"### Response:\")[1])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T16:27:00.225496Z","iopub.execute_input":"2025-05-26T16:27:00.226016Z","iopub.status.idle":"2025-05-26T16:27:30.527668Z","shell.execute_reply.started":"2025-05-26T16:27:00.225993Z","shell.execute_reply":"2025-05-26T16:27:30.526919Z"}},"outputs":[{"name":"stdout","text":"\n<think>\nOkay, so I'm trying to figure out what the medical question is about and how to answer it. Let me start by breaking down the question.\n\nThe patient is a 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night. She's undergoing a gynecological exam and a Q-tip test. The question is about what cystometry would reveal regarding her residual volume and detrusor contractions.\n\nFirst, I need to understand the context. The patient has involuntary urine loss during activities but not at night. That suggests she might have a functional urethral sphincter (FUS) because the sphincter usually controls the flow of urine. Normally, the FUS opens during activity, allowing urine to pass, and closes at rest. The absence of leakage at night might indicate that the sphincter doesn't open as much at night, or perhaps the bladder is more efficiently emptied at rest.\n\nNext, the exam findings are a gynecological exam and a Q-tip test. The Q-tip test is used to assess whether the urethral sphincter is functional. If the test is positive, it means the sphincter is working, and if negative, it's not. In this case, since there's no leakage at night, the Q-tip test would likely come back positive, indicating the FUS is functional.\n\nCystometry is a method to measure residual volume and assess the presence of detrusor contractions. Detrusor contractions are a type of sphincter that open during activity and close at rest, which can contribute to residual volume. In patients with functional FUS, detrusor contractions are usually present and contribute to the measured residual volume.\n\nSo, putting it all together: since the Q-tip test is positive, the FUS is functional, leading to detrusor contractions. The residual volume would be measured as the volume of urine that remains in the bladder after the sphincter opens during activity. The cystometry would show this volume and confirm the presence of detrusor contractions.\n\nWait, but I should make sure about the interpretation. In patients with functional FUS, detrusor contractions are usually present and contribute to residual volume. So, the cystometry result would indicate that there's residual volume and that detrusor contractions are present, likely contributing to the volume measured.\n\nI think that's the correct approach. So, the cystometry would reveal residual volume, and since the Q-tip is positive, detrusor contractions are present.\n</think>\n\nThe cystometry test would reveal residual volume and confirm the presence of detrusor contractions. \n\n**Step-by-Step Explanation:**\n\n1. **Patient Profile:**\n   - 61-year-old woman with a long history of involuntary urine loss during activities but no leakage at night.\n   - Gynecological exam and Q-tip test findings.\n\n2. **Q-tip Test Interpretation:**\n   - The Q-tip test is used to assess the functional state of the urethral sphincter (FUS).\n   - A positive result indicates the FUS is functional, meaning it opens during activity and closes at rest.\n\n3. **Functional Urethral Sphincter (FUS):**\n   - The absence of leakage at night suggests the FUS is functional.\n   - Functional FUS contributes to residual volume and includes detrusor contractions.\n\n4. **Cystometry Interpretation:**\n   - Cystometry measures residual volume by assessing the volume of urine remaining in the bladder.\n   - Functional FUS (as indicated by positive Q-tip test) would show residual volume.\n   - Detrusor contractions are present and contribute to the measured residual volume.\n\n**Conclusion:**\nCystometry would show residual volume, and detrusor contractions would be present, contributing to the measured volume.<｜end▁of▁sentence｜>\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Add the third placeholder for the complex chain of thought column.\ntrain_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the request. \nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \nPlease answer the following medical question. \n\n### Question:\n{}\n\n### Response:\n<think>\n{}\n</think>\n\n{}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T23:45:18.639590Z","iopub.execute_input":"2025-05-26T23:45:18.639899Z","iopub.status.idle":"2025-05-26T23:45:18.645528Z","shell.execute_reply.started":"2025-05-26T23:45:18.639876Z","shell.execute_reply":"2025-05-26T23:45:18.644676Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Download the dataset from HuggingFace\ndataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\",\"en\", split = \"train[0:1000]\",trust_remote_code=True)\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T00:36:31.466167Z","iopub.execute_input":"2025-05-27T00:36:31.466841Z","iopub.status.idle":"2025-05-27T00:36:32.707796Z","shell.execute_reply.started":"2025-05-27T00:36:31.466816Z","shell.execute_reply":"2025-05-27T00:36:32.707235Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['Question', 'Complex_CoT', 'Response'],\n    num_rows: 1000\n})"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"# Signale End of a Text Sequence (Otherwise, leading to incomplete or overly verbose responses)\nEOS_token = tokenizer.eos_token\nif EOS_token is None:\n    EOS_token = tokenizer.pad_token\nif EOS_token is None:\n    EOS_token = \"</s>\"\n\nprint(f\"Using EOS token: {repr(EOS_token)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T00:36:34.480478Z","iopub.execute_input":"2025-05-27T00:36:34.481398Z","iopub.status.idle":"2025-05-27T00:36:34.488422Z","shell.execute_reply.started":"2025-05-27T00:36:34.481367Z","shell.execute_reply":"2025-05-27T00:36:34.487573Z"}},"outputs":[{"name":"stdout","text":"Using EOS token: '<｜end▁of▁sentence｜>'\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"# Define formatting prompt function\ndef formatting_prompts_func(examples):\n    texts = []\n    \n    # Handle both single examples and batches\n    if isinstance(examples[\"Question\"], list):\n        # Batch processing\n        inputs = examples[\"Question\"]\n        cots = examples[\"Complex_CoT\"]\n        outputs = examples[\"Response\"]\n    else:\n        # Single example processing\n        inputs = [examples[\"Question\"]]\n        cots = [examples[\"Complex_CoT\"]]\n        outputs = [examples[\"Response\"]]\n    \n    for input_text, cot, output_text in zip(inputs, cots, outputs):\n        # Skip any None or empty entries\n        if input_text and cot and output_text:\n            text = train_prompt_style.format(input_text, cot, output_text) + EOS_token\n            texts.append(text)\n    \n    return {\"text\": texts}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T00:36:36.416676Z","iopub.execute_input":"2025-05-27T00:36:36.417449Z","iopub.status.idle":"2025-05-27T00:36:36.423515Z","shell.execute_reply.started":"2025-05-27T00:36:36.417421Z","shell.execute_reply":"2025-05-27T00:36:36.422864Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# Update dataset format\ndataset = dataset.map(formatting_prompts_func, batched=True, remove_columns=dataset.column_names) # Apply the same transformation (cookie-making) to every item (dough ball) with batches\ndataset[\"text\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T00:36:47.171632Z","iopub.execute_input":"2025-05-27T00:36:47.172185Z","iopub.status.idle":"2025-05-27T00:36:47.230442Z","shell.execute_reply.started":"2025-05-27T00:36:47.172162Z","shell.execute_reply":"2025-05-27T00:36:47.229637Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9f9e4119ad6432798326e5af94c9475"}},"metadata":{}},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"\"Below is an instruction that describes a task, paired with an input that provides further context. \\nWrite a response that appropriately completes the request. \\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\\n\\n### Instruction:\\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \\nPlease answer the following medical question. \\n\\n### Question:\\nGiven the symptoms of sudden weakness in the left arm and leg, recent long-distance travel, and the presence of swollen and tender right lower leg, what specific cardiac abnormality is most likely to be found upon further evaluation that could explain these findings?\\n\\n### Response:\\n<think>\\nOkay, let's see what's going on here. We've got sudden weakness in the person's left arm and leg - and that screams something neuro-related, maybe a stroke?\\n\\nBut wait, there's more. The right lower leg is swollen and tender, which is like waving a big flag for deep vein thrombosis, especially after a long flight or sitting around a lot.\\n\\nSo, now I'm thinking, how could a clot in the leg end up causing issues like weakness or stroke symptoms?\\n\\nOh, right! There's this thing called a paradoxical embolism. It can happen if there's some kind of short circuit in the heart - like a hole that shouldn't be there.\\n\\nLet's put this together: if a blood clot from the leg somehow travels to the left side of the heart, it could shoot off to the brain and cause that sudden weakness by blocking blood flow there.\\n\\nHmm, but how would the clot get from the right side of the heart to the left without going through the lungs and getting filtered out?\\n\\nHere's where our cardiac anomaly comes in: a patent foramen ovale or PFO. That's like a sneaky little shortcut in the heart between the right and left atria.\\n\\nAnd it's actually pretty common, found in about a quarter of adults, which definitely makes it the top suspect here.\\n\\nSo with all these pieces - long travel, leg clot, sudden weakness - a PFO fits the bill perfectly, letting a clot cross over and cause all this.\\n\\nEverything fits together pretty neatly, so I'd bet PFO is the heart issue waiting to be discovered. Yeah, that really clicks into place!\\n</think>\\n\\nThe specific cardiac abnormality most likely to be found in this scenario is a patent foramen ovale (PFO). This condition could allow a blood clot from the venous system, such as one from a deep vein thrombosis in the leg, to bypass the lungs and pass directly into the arterial circulation. This can occur when the clot moves from the right atrium to the left atrium through the PFO. Once in the arterial system, the clot can travel to the brain, potentially causing an embolic stroke, which would explain the sudden weakness in the left arm and leg. The connection between the recent travel, which increases the risk of deep vein thrombosis, and the neurological symptoms suggests the presence of a PFO facilitating a paradoxical embolism.<｜end▁of▁sentence｜>\""},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"# Filter out any empty texts\ndataset = dataset.filter(lambda x: len(x[\"text\"]) > 1000)\n\nprint(f\"Dataset size after processing: {len(dataset)}\")\nprint(\"Sample text length:\", len(dataset[\"text\"][0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T00:36:53.780143Z","iopub.execute_input":"2025-05-27T00:36:53.780450Z","iopub.status.idle":"2025-05-27T00:36:53.825158Z","shell.execute_reply.started":"2025-05-27T00:36:53.780430Z","shell.execute_reply":"2025-05-27T00:36:53.824382Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf70ae1cf95c4f8c97a0853861f879a6"}},"metadata":{}},{"name":"stdout","text":"Dataset size after processing: 1000\nSample text length: 3055\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"# Apply LoRA (Low-rank Adaptation) fine-tuning to the model\nmodel_lora = FastLanguageModel.get_peft_model(\n    model,\n    r = 8, # Higher r allows the LoRA adapters to capture more complex changes, but it also increases the number of trainable parameters and memory usage.\n    target_modules = [\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        #\"up_proj\",\n        #\"#down_proj\",\n        #\"embed_tokens\", \n        #\"lm_head\"\n    ],\n    lora_alpha= 16, # Higher r allows the LoRA adapters to capture more complex changes, but it also increases the number of trainable parameters and memory usage.\n    lora_dropout=0, # no dropout is applied\n    bias= \"none\", # No bias terms will be trained\n    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\". works by recomputing activations during the backward pass instead of storing them all in memory during the forward pass. This saves significant GPU memory at the cost of slightly more computation time.\n    random_state=3407,\n    use_rslora=False,  # 'rslora' stands for Rank-Stabilized LoRA that is a variant of LoRA\n    loftq_config=None,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T00:37:17.586868Z","iopub.execute_input":"2025-05-27T00:37:17.587173Z","iopub.status.idle":"2025-05-27T00:37:24.725271Z","shell.execute_reply.started":"2025-05-27T00:37:17.587151Z","shell.execute_reply":"2025-05-27T00:37:24.724598Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"import os\n\nos.environ[\"UNSLOTH_DISABLE_FAST_CROSS_ENTROPY\"] = \"1\"\nos.environ[\"UNSLOTH_DISABLE_FAST_CROSS_ENTROPY_LOSS\"] = \"1\" \nos.environ[\"DISABLE_FUSED_CROSS_ENTROPY\"] = \"1\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T00:37:24.726516Z","iopub.execute_input":"2025-05-27T00:37:24.727440Z","iopub.status.idle":"2025-05-27T00:37:24.732427Z","shell.execute_reply.started":"2025-05-27T00:37:24.727418Z","shell.execute_reply":"2025-05-27T00:37:24.731766Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T23:45:51.880375Z","iopub.execute_input":"2025-05-26T23:45:51.880688Z","iopub.status.idle":"2025-05-26T23:45:51.885956Z","shell.execute_reply.started":"2025-05-26T23:45:51.880667Z","shell.execute_reply":"2025-05-26T23:45:51.885260Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Initialize the fine-tuning trainer\n\"\"\"trainer = SFTTrainer(\n    model = model_lora,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    dataset_text_field=\"text\", # Contains the actual formatted text that should be tokenized and fed to the model\n    max_seq_length=1024, # Use 2 parallel processes (CPU cores) to prepare the dataset\n    dataset_num_proc=1,\n    packing=False,\n    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n\n    args=TrainingArguments(\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=4, # Accumulate gradients for 4 steps before actually updating the model's weights.\n        # Use num_train_epochs = 1, warmup_ratio for full training runs! The number of steps over which the learning rate will gradually increase from 0 to its learning_rate value\n        warmup_steps=2,\n        max_steps=30,# The total number of optimization steps to perform during training.\n        learning_rate=1e-4,#This determines how large of a step the optimizer takes when updating model weights based on gradients\n        fp16=True,\n        bf16=False,\n        logging_steps=5,\n        optim=\"adamw_torch\", # Use an 8-bit optimized version of the AdamW optimizer (from the bitsandbytes library), significantly reducing the memory footprint of the optimizer's states\n        weight_decay=0.01, # A form of regularization (L2 regularization) that penalizes large weights, helping to prevent overfitting.\n        lr_scheduler_type=\"linear\", # Linearly decay from its peak (after warm-up) down to 0\n        seed=3407,\n        output_dir=\"outputs\", \n        dataloader_drop_last=True,      \n        remove_unused_columns=True,  # Changed to True to clean up data\n        dataloader_pin_memory=False,  # Disable pin memory to avoid issues\n    ),\n)\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-24T21:01:22.693293Z","iopub.execute_input":"2025-05-24T21:01:22.694036Z","iopub.status.idle":"2025-05-24T21:01:23.937560Z","shell.execute_reply.started":"2025-05-24T21:01:22.694010Z","shell.execute_reply":"2025-05-24T21:01:23.937005Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"]:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9472beb9b8a74827b1bd6302323c9a5f"}},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"os.environ['UNSLOTH_RETURN_LOGITS'] = '1'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T00:37:24.986838Z","iopub.execute_input":"2025-05-27T00:37:24.987620Z","iopub.status.idle":"2025-05-27T00:37:24.992509Z","shell.execute_reply.started":"2025-05-27T00:37:24.987593Z","shell.execute_reply":"2025-05-27T00:37:24.991645Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"# Use standard Transformers Trainer instead of SFTTrainer\nfrom transformers import Trainer\nimport torch.nn as nn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T23:45:58.218755Z","iopub.execute_input":"2025-05-26T23:45:58.219565Z","iopub.status.idle":"2025-05-26T23:45:58.224192Z","shell.execute_reply.started":"2025-05-26T23:45:58.219540Z","shell.execute_reply":"2025-05-26T23:45:58.223300Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Custom trainer class \nclass StandardTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        # Check if labels exist, if not create them from input_ids\n        if \"labels\" not in inputs or inputs[\"labels\"] is None:\n            inputs[\"labels\"] = inputs[\"input_ids\"].clone()\n        \n        labels = inputs[\"labels\"]\n        outputs = model(**inputs)\n        logits = outputs.get(\"logits\")\n        \n        # Use standard PyTorch CrossEntropyLoss\n        loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n        shift_logits = logits[..., :-1, :].contiguous()\n        shift_labels = labels[..., 1:].contiguous()\n        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n        \n        return (loss, outputs) if return_outputs else loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T00:37:29.792816Z","iopub.execute_input":"2025-05-27T00:37:29.793426Z","iopub.status.idle":"2025-05-27T00:37:29.801157Z","shell.execute_reply.started":"2025-05-27T00:37:29.793400Z","shell.execute_reply":"2025-05-27T00:37:29.800225Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"# Tokenize dataset properly with labels\ndef tokenize_function(examples):\n    tokenized = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n    return tokenized\n\ntokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T00:37:31.177372Z","iopub.execute_input":"2025-05-27T00:37:31.177926Z","iopub.status.idle":"2025-05-27T00:37:32.522674Z","shell.execute_reply.started":"2025-05-27T00:37:31.177898Z","shell.execute_reply":"2025-05-27T00:37:32.522043Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cfb1532bd0946629b54558de87f6443"}},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"\n# Create standard trainer\ntrainer = StandardTrainer(\n    model=model_lora,\n    tokenizer=tokenizer,\n    train_dataset=tokenized_dataset,\n    args=TrainingArguments(\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        warmup_steps=5,\n        max_steps=60,\n        learning_rate=5e-4,\n        fp16=True,\n        bf16=False,\n        logging_steps=5,\n        optim=\"adamw_torch\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"linear\",\n        seed=3407,\n        output_dir=\"outputs\",\n        dataloader_drop_last=True,\n        remove_unused_columns=False,\n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T00:37:36.579219Z","iopub.execute_input":"2025-05-27T00:37:36.579965Z","iopub.status.idle":"2025-05-27T00:37:36.626223Z","shell.execute_reply.started":"2025-05-27T00:37:36.579932Z","shell.execute_reply":"2025-05-27T00:37:36.625603Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/185008111.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `StandardTrainer.__init__`. Use `processing_class` instead.\n  trainer = StandardTrainer(\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"# Train with standard trainer\ntrainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T00:37:40.706396Z","iopub.execute_input":"2025-05-27T00:37:40.707078Z","iopub.status.idle":"2025-05-27T00:43:01.189260Z","shell.execute_reply.started":"2025-05-27T00:37:40.707054Z","shell.execute_reply":"2025-05-27T00:43:01.188563Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 2\n   \\\\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 60\nO^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n \"-____-\"     Trainable parameters = 13,934,592/5,000,000,000 (0.28% trained)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [60/60 05:12, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>5</td>\n      <td>6.891100</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>6.667900</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>6.866300</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>6.749700</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>6.991100</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>6.812200</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>6.794700</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>6.772000</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>6.938900</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>6.769300</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>6.528400</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>6.579600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"# Get summary\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T00:43:17.481440Z","iopub.execute_input":"2025-05-27T00:43:17.481726Z","iopub.status.idle":"2025-05-27T00:43:17.957421Z","shell.execute_reply.started":"2025-05-27T00:43:17.481706Z","shell.execute_reply":"2025-05-27T00:43:17.956707Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▂▃▃▄▄▁▁▂▂▂▃▃▁▁▂▂▂▃▃▃▃▄▄▄▄▁▂▃▃▄▄▅▆▆▇▇██</td></tr><tr><td>train/global_step</td><td>▂▃▄▅▇▇▁▂▂▃▄▄▄▁▂▂▃▄▄▅▅▆▇▇██▁▂▂▃▄▄▅▅▆▇▇██</td></tr><tr><td>train/grad_norm</td><td>█▅▅▅▄▆▅▅▄▄▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▂▂▂▁▁▇▇▆▄▃▁▇█▇▇▆▅▅▄▃▂▂▁▇█▇▇▆▅▅▄▃▂▂▁</td></tr><tr><td>train/loss</td><td>█▆▅▄▄▃▄▄▄▄▄▁▁▂▃▃▄▄▄▄▄▄▄▃▃▃▃▄▃▃▃▄▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>5281940546519040.0</td></tr><tr><td>train/epoch</td><td>0.96</td></tr><tr><td>train/global_step</td><td>60</td></tr><tr><td>train/grad_norm</td><td>0.66169</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>6.5796</td></tr><tr><td>train_loss</td><td>6.78009</td></tr><tr><td>train_runtime</td><td>318.1776</td></tr><tr><td>train_samples_per_second</td><td>3.017</td></tr><tr><td>train_steps_per_second</td><td>0.189</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">zesty-thunder-3</strong> at: <a href='https://wandb.ai/hanksouyang5155-northeastern-university/DeepSeek-R1-Distill-Qwen-1.5B%20for%20medical%20use/runs/iye2gfve?apiKey=8f86d9dcb091e484f533faa0c533e606d51eeb86' target=\"_blank\">https://wandb.ai/hanksouyang5155-northeastern-university/DeepSeek-R1-Distill-Qwen-1.5B%20for%20medical%20use/runs/iye2gfve?apiKey=8f86d9dcb091e484f533faa0c533e606d51eeb86</a><br> View project at: <a href='https://wandb.ai/hanksouyang5155-northeastern-university/DeepSeek-R1-Distill-Qwen-1.5B%20for%20medical%20use?apiKey=8f86d9dcb091e484f533faa0c533e606d51eeb86' target=\"_blank\">https://wandb.ai/hanksouyang5155-northeastern-university/DeepSeek-R1-Distill-Qwen-1.5B%20for%20medical%20use?apiKey=8f86d9dcb091e484f533faa0c533e606d51eeb86</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250526_234341-iye2gfve/logs</code>"},"metadata":{}}],"execution_count":47},{"cell_type":"code","source":"# Model inference after fine-tuning\nquestion_1 = \"\"\"A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing \n            but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings, what would cystometry most \n            likely reveal about her residual volume and detrusor contractions?\"\"\"\n\n# Load the inference model using FastLanguageModel\nFastLanguageModel.for_inference(model)  \n\n# Tokenize the input question with a specific prompt format and run with GPU\ninputs_1 = tokenizer([prompt_style.format(question_1, \"\")], return_tensors=\"pt\").to(\"cuda\")\n\n# Generate a response using LoRA\noutputs_1 = model.generate(\n    input_ids=inputs_1.input_ids, # TOkenized input IDs\n    attention_mask=inputs_1.attention_mask, # Attention mask for padding handling\n    max_new_tokens=1200, # Maximum length for generated response\n    use_cache=True, # Enable cache for efficient generation\n)\n\n# Decode the generated response from tokenized format to readable text\nresponse_1 = tokenizer.batch_decode(outputs_1)\n\n# Extract and print the part after '### Response'\nprint(response_1[0].split(\"### Response:\")[1])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T00:44:02.111527Z","iopub.execute_input":"2025-05-27T00:44:02.111837Z","iopub.status.idle":"2025-05-27T00:44:37.148828Z","shell.execute_reply.started":"2025-05-27T00:44:02.111816Z","shell.execute_reply":"2025-05-27T00:44:37.148000Z"}},"outputs":[{"name":"stdout","text":"\n<think>\nAlright, so we have a 61-year-old woman here. She's got a long history of losing urine when she coughs or sneezes, but she doesn't leak at night. That's interesting. I know that could mean something's off with her kidneys or maybe her bladder. Hmm, let's think about what could cause that.\n\nNow, she's got a gynecological exam and a Q-tip test. The Q-tip test is pretty standard for checking bladder function. It's like checking if the bladder can hold enough fluid when you push it out. But, she's got a long history of urine loss, so that's a big clue. If her bladder is not holding enough, it could explain the leaky night too.\n\nOkay, so if the bladder isn't holding enough fluid, that usually means there's something wrong with the detrusor muscles. They're responsible for holding onto the fluid in the bladder, and if they're not working right, the bladder could leak.\n\nNow, thinking about cystometry, that's a test we use to look at how much fluid is in the bladder. It's usually used to see if the bladder is losing too much fluid. But, if the bladder is leaking, cystometry might not show much because it's focused on holding fluid.\n\nIn this case, she's losing urine when she's active, which is a big red flag for losing fluid. Plus, she's losing it only during activities like coughing or sneezing. That sounds like she's losing fluid when she's not in the normal state of her bladder.\n\nSo, putting it all together, she's likely losing fluid because her detrusor muscles aren't working properly. That would mean cystometry wouldn't show much because cystometry is all about holding fluid, not losing it. Therefore, cystometry might not show significant losses or changes in fluid levels.\n\nHmm, I'm thinking that cystometry would probably show less fluid loss because she's losing fluid during activities but not at night. That makes sense, right? So, cystometry would likely reveal less fluid loss and might show no significant changes in fluid levels due to detrusor issues. Yeah, that fits with what we've been thinking.\n</think>\n\nThe cystometry would most likely reveal less fluid loss due to detrusor contraction issues. This is because cystometry focuses on holding fluid, so if the bladder is leaking, cystometry wouldn't show significant fluid loss. The leaky night and urine loss during activities suggest detrusor dysfunction, which impacts fluid holding. Therefore, cystometry would likely show less fluid loss and possibly no significant changes in fluid levels.<｜end▁of▁sentence｜>\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"# Test 2nd example\nquestion_2 = \"A 59-year-old man presents with a fever, chills, night sweats, and generalized fatigue, and is found to have a 12 mm vegetation on the aortic valve. Blood cultures indicate gram-positive, catalase-negative, gamma-hemolytic cocci in chains that do not grow in a 6.5% NaCl medium. What is the most likely predisposing factor for this patient's condition?\"\n\ninputs_2 = tokenizer([prompt_style.format(question_2, \"\")], return_tensors=\"pt\").to(\"cuda\")\n\noutputs_2 = model.generate(\n    input_ids=inputs_2.input_ids, # TOkenized input IDs\n    attention_mask=inputs_2.attention_mask, # Attention mask for padding handling\n    max_new_tokens=1200, # Maximum length for generated response\n    use_cache=True, # Enable cache for efficient generation\n)\n\n# Decode the generated response from tokenized format to readable text\nresponse_2 = tokenizer.batch_decode(outputs_2)\n\n# Extract and print the part after '### Response'\nprint(response_2[0].split(\"### Response:\")[1])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T19:42:25.807355Z","iopub.execute_input":"2025-05-26T19:42:25.807625Z","iopub.status.idle":"2025-05-26T19:42:50.839131Z","shell.execute_reply.started":"2025-05-26T19:42:25.807606Z","shell.execute_reply":"2025-05-26T19:42:50.838484Z"}},"outputs":[{"name":"stdout","text":"\n<think>\nAlright, let's break this down. We've got a 59-year-old guy who's showing signs of a severe illness: fever, chills, night sweats, and fatigue. That's a lot to take in. And then there's this 12 mm vegetation on his aortic valve. Hmm, that's interesting because it's not a common thing, especially in someone who's already got these symptoms.\n\nNow, let's think about the blood cultures. They're gram-positive, which means they're not white blood cells. And they're catalase-negative, which is a clue. Catalase is an enzyme that breaks down lactose into glucose and galactose, so if this enzyme is negative, it means lactose is not being broken down. That's a big red flag.\n\nThe fact that the cocci chains don't grow in a 6.5% NaCl medium means they're not resistant to this kind of acid. It's like they're not stable and can be hydrolyzed. This is usually a sign of some kind of acid resistance issue.\n\nPutting all these clues together, it's starting to sound like we're dealing with something like endocarditis. Endocarditis involves inflammation and infection of the heart valves. And when you have this kind of vegetation on the valve, it's a big clue that the valves might be inflamed.\n\nSo, considering all this, it seems pretty clear that the most likely thing going on is endocarditis. It fits the symptoms, the blood culture results, and the presence of the vegetation. Yeah, I think we're on the right track here.\n</think>\n\nThe most likely predisposing factor for this patient's condition is endocarditis. The symptoms of fever, chills, night sweats, and fatigue, along with the presence of 12 mm vegetation on the aortic valve, are consistent with the presentation of endocarditis. The gram-positive nature of the blood cultures, the catalase-negative result, and the lack of resistance to acid in the 6.5% NaCl medium further support this diagnosis. Endocarditis is characterized by valve inflammation and infection, which aligns perfectly with the clinical presentation and laboratory findings.<｜end▁of▁sentence｜>\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"# Save the model locally\nnew_model_local = \"DeepSeek-R1-Medical-COT-1.5B\"\nmodel.save_pretrained(new_model_local) # Only saves the LoRA adapter weights, training later or to experiment with different base models.\ntokenizer.save_pretrained(new_model_local) # Essential for converting text to numerical IDs\n\n# Merge the newly trained LoRA adapter weights directly into the original frozen base model weights\nmodel.save_pretrained_merged(new_model_local, tokenizer, save_method = \"merged_16bit\",)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T01:10:13.815436Z","iopub.execute_input":"2025-05-27T01:10:13.815739Z","iopub.status.idle":"2025-05-27T01:10:55.764699Z","shell.execute_reply.started":"2025-05-27T01:10:13.815722Z","shell.execute_reply":"2025-05-27T01:10:55.764014Z"}},"outputs":[{"name":"stderr","text":"Unsloth: You have 2 CPUs. Using `safe_serialization` is 10x slower.\nWe shall switch to Pytorch saving, which might take 3 minutes and not 30 minutes.\nTo force `safe_serialization`, set it to `None` instead.\nUnsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\nmodel which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\nUnsloth: Will remove a cached repo with size 1.8G\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 18.29 out of 31.35 RAM for saving.\nUnsloth: Saving model... This might take 5 minutes ...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 28/28 [00:00<00:00, 44.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Saving tokenizer... Done.\nUnsloth: Saving DeepSeek-R1-Medical-COT-1.5B/pytorch_model.bin...\nDone.\n","output_type":"stream"}],"execution_count":49}]}